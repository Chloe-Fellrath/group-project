---
title: "Data"
format:
  html: default
  pdf:
    toc: false
    echo: true          # hide code in the PDF
    code-overflow: wrap  # wrap long code lines
---

## Sources
For this project, we are using public and open-source data via Spotify APIs. We selected 8 playlists from the Billboard Year-End Top Charts which we have merged to have only one data frame. Compared to other data sources, we chose APIs because they provide access to a large and varied set of music-related features while allowing greater flexibility and control during the data collection process.
Some challenges that we can encounter with using Spotify’s API:

Certain information, such as exact play counts for tracks or compilations, is no longer publicly available. These limitations could be due to licensing restrictions, copyright concerns, or changes in Spotify’s data policies.
The popularity variable is somewhat coarse and could be biased through the decades: "The artist's popularity is calculated from the popularity of all the artist's tracks." (@spotify)
"The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are. Generally speaking, songs that are being played a lot now will have a higher popularity than songs that were played a lot in the past.
Duplicate tracks (e.g. the same track from a single and an album) are rated independently. Artist and album popularity are derived mathematically from track popularity. Note: the popularity value may lag actual popularity by a few days: the value is not updated in real time." (@spotify)
Similarly, Spotify’s genre classification is not fully transparent. It relies on internal algorithms and curation processes, which may lead to inconsistencies across tracks or artists. Since genres are assigned at the artist level rather than the track level, some variation occurs when artists experiment with different styles. Nevertheless, the classification is generally sufficient for analysis.
Additionally, Spotify does not provide information about an artist’s gender, so we cross-referenced this metadata using MusicBrainz.
Despite these limitations, Spotify’s APIs remain a rich and accessible source of music data, particularly for track metadata and artist information.

::: {.callout-important}

## Data Source Documentation

Data Source: cleaned_final_dataset.csv
data/processed/
:::

## Description
From Spotify APIs, we collected the following variables for each track:
- track_id: Unique identifier for the track (tr.get("id")) : string (e.g. "6dOtVTDdiauQNBQEDOtlAB")
- track_name: Track title (tr.get("name")) : string (e.g. "golden")
- track_popularity: Popularity score (tr.get("popularity")) : integer (0-100)
- duration_ms: Track duration in milliseconds (tr.get("duration_ms")) : integer (e.g. 210373)
- artist_names: Names of the performing artists : string (e.g. "billie eilish")
- artist_ids: Spotify IDs of the artists : string (e.g. "6qqNVTkY8uBg9cP3Jd7DAH")
- album_id: Unique album identifier : string (e.g. "14JkAa6IiFaOh5s0nMyMU9")
- album_name: Album title : string (e.g. "hit me hard and soft")
- album_release_date: Release date of the album : string (e.g. "2024-05-17 00:00:00+00:00")
- album_total_tracks: Number of tracks on the album : integer (e.g. 10)
- genre: the musical genre of the track: string (e.g. "pop")

From MusicBrainz metadata, we obtained the gender of the primary (first-listed) performing artist.
Possible values include 'female', 'male', 'non-binary', 'group' (for collaborations), and 'unknown' when the information was missing.

The dataset is stored in .csv format. Preprocessing included:
- Removing duplicate tracks
- Standardizing date formats for album_release_date
- Normalizing text fields (lowercase, trimmed whitespace)
- Converting numeric fields
- Filling missing values where appropriate
After merging and cleaning, the dataset contains 1,239 records and 13 variables stored in CSV format for ease of processing in Python.

::: {.callout-note}
## Dataset Overview Template

Summary:
- **Number of observations**: 1239 rows
- **Number of variables**: 13 columns
- **Time period**: 1957-12-02 to 2025-09-26
- **Geographic coverage**: Global
- **Key variables**:
For our analysis, we will focus on a subset of these variables as they allow us to capture artists identity, temporal dynamics and popularity value:
- track_popularity
- genre
- gender
- release_year - new numeric variable to support time-based analysis

:::

## Loading Data

::: {.callout-tip}
## Loading Data Best Practices

1. **Use relative paths** via `project_root` for reproducibility
2. **Check data types** after loading
3. **Inspect the first few rows** to verify correct loading
4. **Document any loading parameters** (e.g., encoding, delimiter)

:::

```{python}
#| eval: false
#| output: true

#| label: load-data
# Code visibility controlled by format settings in report.qmd
#| output: true

"""Example of loading data using pandas with proper path handling."""

import pandas as pd
from IPython.display import display
from quarto_runtime import project_root

# Define the path to raw data using project_root
# This ensures the code works regardless of where it's run from
raw_data_path = project_root / "data" / "raw" / "data_raw.csv"

# Load the data
data = pd.read_csv(raw_data_path, skipinitialspace=True)

# Basic data inspection
print(f"Dataset shape: {data.shape[0]} rows × {data.shape[1]} columns")
print(f"\nColumn names: {list(data.columns)}")
print(f"\nData types:\n{data.dtypes}")

# Display first few rows
print("\nFirst 5 rows:")
display(data.head())
```
::: {.callout-tip}
## Alternative Data Sources

- **CSV files**: `pd.read_csv(cleaned_final_dataset.csv)`
(- **APIs**: Using `requests` library + `pd.DataFrame(data)`)

:::

## Wrangling

### General Transformations
We performed several preprocessing steps to ensure data quality. These included:
- Cleaning column names
- Handling missing values
- Standardizing text and categorical variables
- Transforming dates and numeric fields
- Validating consistency across merged sources

::: {.callout-caution}
## Documenting Our Transformations
1. Clean column names: Ensures consistency and avoids errors in referencing columns in analysis.
```{python}
#| eval: false
import pandas as pd

COLS = {
    "track_id":           "track_id",
    "track_name":         "track_name",
    "artist_names":       "artist_names",
    "album_release_date": "album_release_date",
    "track_popularity":   "track_popularity",
}
COLS
```
2. Handling missing values: Missing values can break numeric operations or models.
```{python}
#| eval: false
import pandas as pd
# Text
text_cols = ['artist_names', 'track_name', 'album_name', 'genre', 'album_release_date', 'gender']
for col in text_cols:
df[col] = df[col].fillna('unknown').str.lower()
# Numeric
numeric_cols = ['track_popularity', 'duration_ms', 'album_total_tracks', 'release_year']
for col in numeric_cols:
if col in df.columns:
df[col] = df[col].fillna(0)
df[col] = df[col].astype('int64')
# Mark groups in gender
df.loc[df['artist_names'].str.contains(';'), 'gender'] = 'group'
return df
df = pd.read_csv("cleaned_final_dataset.csv")
print(df.head())
```
3. Standardizing text and categorical variables: Categorical/text fields may need consistent formatting.
```{python}
#| eval: false
# Remove spaces and convert to lowercase
df['artist'] = df['artist_names'].str.strip().str.title()
df['track_name'] = df['track_name'].str.strip().str.lower()
```
4. Transforming dates and numeric fields: Proper datetime objects allow easy date calculations, filtering, and feature extraction.
```{python}
#| eval: false
# Dates → proper datetime + helper year column
df[COLS["album_release_date"]] = pd.to_datetime(
    df[COLS["album_release_date"]], errors="coerce", utc=True
)
df["release_year"] = df[COLS["album_release_date"]].dt.year
```
5. Validating consistency across merged sources: Ensures transformations did not introduce errors.
```{python}
#| eval: false
print(df.head())  # shows the first 5 rows
print(df.info())  # shows column types and missing counts
print(df.isnull().sum()) # shows how many missing values are in each column
print(df['release_year'].unique())  # Make sure year column looks correct
```
## Alignment with Research Questions
- All cleaning steps were focused on ensuring reliable measurement of artist identity, temporal dynamics, and track popularity, which are central to the analysis.
- By prioritizing critical variables and preserving real variation, the dataset remains representative and suitable for exploring patterns in musical trends.
:::

### Spotting Mistakes and Missing Data
Missing values were identified in the release_year_album column, which also affected the derived release_year column.
For categorical variables such as gender and genre, missing values occurred due to incomplete open-source metadata—some artists were not present in the source datasets. Since our analysis requires full coverage of the dataset to avoid empty sets, we imputed these missing entries with 'unknown'.
For numerical variables, such as track_popularity, missing values were imputed with 0. This choice reflects the fact that popularity is linked to the total number of plays, which is currently coarse-grained, and ensures that calculations and analyses can proceed without errors.

::: {.callout-warning}
We decided to handle these missing values using imputation, rather than deletion, because the other columns associated with these rows remained consistent and valuable for analysis.
By applying these imputation strategies:
- We preserved all rows in the dataset (no deletion), maintaining dataset completeness.
- We ensured that numeric computations and type conversions (e.g., int64) are valid.
- Categorical analyses can proceed without errors, with missing values clearly flagged as 'unknown'.
This approach provides a reproducible, consistent, and practical solution for handling missing data, allowing reliable downstream analysis while maintaining data integrity.
```{python}
#| eval: false
# Impute missing numeric values with 0 to ensure calculations and type conversions work
df['track_popularity'] = df['track_popularity'].fillna(0).astype('int64')
# Convert album release date to datetime (invalid dates become 'unknown')
df['album_release_date'] = pd.to_datetime(df['album_release_date'], errors='coerce').dt.year
# Extract release year as integer; fill missing years with 0
df['release_year'] = df['album_release_date'].dt.year.fillna(0).astype('int64')
print(df.isnull().sum())  # Check that there are no remaining missing values in numeric columns
```
:::

### Listing Anomalies and Outliers
We examined the dataset for anomalies and outliers using visualizations such as scatter-plots (e.g., for track duration). While a few extreme values were identified, all of them appeared to be plausible, real, but unusual observations rather than errors. Therefore, these outliers were retained in the dataset.
Since the outliers did not represent data errors or patterns that would meaningfully impact our analysis, they were not considered particularly informative for our study.
Retaining them is consistent with our research objectives, as these extreme values still reflect genuine variations in track duration and popularity, which are relevant for understanding trends in musical characteristics and artist behavior over time.
::: {.callout-tip}
## Outlier Detection Methods
- Visual inspection: scatter-plots
- Statistical methods: IQR-based thresholds
- Domain knowledge validation: distinguishing real extremes from errors
```{python}
#| eval: false
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Loading dataset
data = pd.read_csv("data/processed/cleaned_final_dataset.csv")

# Function to detect outliers using IQR
def detect_outliers(df, col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    return df[(df[col] < lower) | (df[col] > upper)]

# Example: highlight outliers in track_popularity vs duration_ms
outliers = detect_outliers(data, "duration_ms")  # we focus on duration_ms here

plt.figure(figsize=(10,6))
sns.scatterplot(data=data, x="duration_ms", y="track_popularity", alpha=0.5, label="Normal")
sns.scatterplot(data=outliers, x="duration_ms", y="track_popularity", color="red", label="Outliers", s=60)
plt.title("Track Popularity vs Duration (Outliers Highlighted)")
plt.xlabel("Duration (ms)")
plt.ylabel("Popularity (0-100)")
plt.legend()
plt.show()
```
:::

::: {.callout-tip}

::: {.callout-important}

::: {.callout-note}
