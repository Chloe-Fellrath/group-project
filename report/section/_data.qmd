# Data

## Sources
For this project, we are using public and open-source data via Spotify APIs. We selected 8 playlists from the Billboard Hot 100 Top Charts which we have merged to have only one data frame. Compared to other data sources, we chose APIs because they provide access to a large and varied set of music-related features while allowing greater flexibility and control during the data collection process.
Some challenges that we can encounter with using Spotify’s API:

Certain information, such as exact play counts for tracks or compilations, is no longer publicly available. These limitations could be due to licensing restrictions, copyright concerns, or changes in Spotify’s data policies.
The popularity variable is somewhat coarse and could be biased through the decades: "The artist's popularity is calculated from the popularity of all the artist's tracks." - https://developer.spotify.com/documentation/web-api/reference/get-an-artist
"The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are. Generally speaking, songs that are being played a lot now will have a higher popularity than songs that were played a lot in the past.
Duplicate tracks (e.g. the same track from a single and an album) are rated independently. Artist and album popularity are derived mathematically from track popularity. Note: the popularity value may lag actual popularity by a few days: the value is not updated in real time." - https://developer.spotify.com/documentation/web-api/reference/get-several-tracks
Similarly, Spotify’s genre classification is not fully transparent. It is based on internal algorithms and curation processes and may not be consistent across all tracks or artists. Additionally, its genre classification is based on the 'artist' and not the 'track', thus it is sometimes inconsistent since artists tend to change genre specificity. However, it is generally adequate.
Furthermore, Spotify does not disclose an artist's gender. So we had to cross-reference with another metadata (MusicBrainz).
Despite these challenges, Spotify APIs remain a rich and accessible source for music data, particularly features like track metadata and artist information.

::: {.callout-important}

## Data Source Documentation

Data Source: cleaned_final_dataset.csv
data/processed/
:::

## Description
From Spotify APIs, we collected the following variables for each track:
- track_id: Unique identifier for the track (tr.get("id")) : string (e.g. "6dOtVTDdiauQNBQEDOtlAB")
- track_name: Track title (tr.get("name")) : string (e.g. "golden")
- track_popularity: Popularity score (tr.get("popularity")) : integer (0-100)
- duration_ms: Track duration in milliseconds (tr.get("duration_ms")) : integer (e.g. 210373)
- artist_names: Names of the performing artists : string (e.g. "billie eilish")
- artist_ids: Spotify IDs of the artists : string (e.g. "6qqNVTkY8uBg9cP3Jd7DAH")
- album_id: Unique album identifier : string (e.g. "14JkAa6IiFaOh5s0nMyMU9")
- album_name: Album title : string (e.g. "hit me hard and soft")
- album_release_date: Release date of the album : string (e.g. "2024-05-17 00:00:00+00:00")
- album_total_tracks: Number of tracks on the album : integer (e.g. 10)
- genre: the musical genre of the track: string (e.g. "pop")

From MusicBrainz metadata, we collected 'gender': gender of the main (first) performing artist (e.g. 'female', 'male', 'non-binary', 'group' - for collaborations, and 'unknown' when the value was missing.

The dataset is stored in .csv format. Preprocessing included:
- Removing duplicate tracks
- Standardizing date formats for album_release_date
- Normalizing text fields (lowercase, trimmed whitespace)
- Converting numeric fields
- Filling missing values where appropriate
After merging and cleaning, the dataset contains 1,238 records stored in CSV format for ease of processing in Python.

::: {.callout-note}
## Dataset Overview Template

Summary:
- **Number of observations**: 1238 rows
- **Number of variables**: 12 columns
- **Time period**: 1957-12-02 to 2025-09-26
- **Geographic coverage**: Global
- **Key variables**:
For our analysis, we will focus on a subset of these variables as they allow us to capture artists identity, temporal dynamics and popularity value:
- track_name
- track_popularity
- artist_names
- album_release_date
- genre – the musical genre of the track
- gender – the gender of the main performing artist(s): determined by cross-referencing artist metadata from public sources.

:::

## Loading Data

::: {.callout-tip}
## Loading Data Best Practices

1. **Use relative paths** via `project_root` for reproducibility
2. **Check data types** after loading
3. **Inspect the first few rows** to verify correct loading
4. **Document any loading parameters** (e.g., encoding, delimiter)

:::

```{python}
#| label: load-data
# Code visibility controlled by format settings in report.qmd
#| output: true

"""Example of loading data using pandas with proper path handling."""

import pandas as pd
from IPython.display import display
from quarto_runtime import project_root

# Define the path to raw data using project_root
# This ensures the code works regardless of where it's run from
raw_data_path = project_root / "data" / "raw" / "data_raw.csv"

# Load the data
data = pd.read_csv(raw_data_path, skipinitialspace=True)

# Basic data inspection
print(f"Dataset shape: {data.shape[0]} rows × {data.shape[1]} columns")
print(f"\nColumn names: {list(data.columns)}")
print(f"\nData types:\n{data.dtypes}")

# Display first few rows
print("\nFirst 5 rows:")
display(data.head())
```

::: {.callout-tip}
## Alternative Data Sources

- **CSV files**: `pd.read_csv(cleaned_final_dataset.csv)`
(- **APIs**: Using `requests` library + `pd.DataFrame(data)`)

:::

## Wrangling

### General Transformations
We performed several preprocessing steps to ensure data quality. These included:
- cleaning column names
- handling missing values
- converting text and categorical variables
- transforming dates and numeric fields
- validating consistency across merged sources

::: {.callout-caution}
## Document Your Transformations

Every transformation should be:
1. Clean column names: it is important for referencing, to make sure the data is coherent and proceed that data analysis.
```{python}
import pandas as pd

COLS = {
    "track_id":           "track_id",
    "track_name":         "track_name",
    "artist_names":       "artist_names",
    "album_release_date": "album_release_date",
    "genre":              "genre",                 # you don't have it (that's fine)
    "track_popularity":   "track_popularity",
}
COLS

df = pd.read_csv("filename.csv")
df.head()

1. **Justified**: Explain why it's needed
2. **Documented**: Clear code comments
3. **Reproducible**: Can be run from scratch
4. **Validated**: Check the results make sense
:::

### Spotting Mistakes and Missing Data
Discuss any identified mistakes or issues with missing data and describe your approach to handling them.

::: {.callout-warning}
## Missing Data Strategies

Different approaches for different situations:

- **Deletion**: Remove rows/columns with missing values (when few missing)
- **Imputation**: Fill with mean, median, mode, or advanced methods
- **Flagging**: Create indicator variables for missingness
- **Model-based**: Use algorithms that handle missing values

**Document your choice** and justify why it's appropriate for your data!
:::

### Listing Anomalies and Outliers
Identify any anomalies or outliers discovered, along with your approach to assessing their impact.

::: {.callout-tip}
## Outlier Detection Methods

- **Visual inspection**: Box plots, scatter plots
- **Statistical methods**: Z-scores, IQR method
- **Domain knowledge**: What values are impossible or implausible?

Remember: Not all outliers should be removed! They might be:

- **Errors**: Data entry mistakes (should be corrected/removed)
- **Valid extremes**: Real but unusual observations (should be kept)
- **Key insights**: The most interesting part of your data!
:::

```{python}
#| label: data-cleaning
# Code visibility controlled by format settings in report.qmd
#| output: true

"""Example of data cleaning with pandas - proper transformations."""

import pandas as pd
import numpy as np
from IPython.display import display
from quarto_runtime import project_root

# Load raw data using the same approach as above
raw_data = project_root / "data" / "raw" / "data_raw.csv"
data = pd.read_csv(raw_data, skipinitialspace=True)

# Clean column names (lowercase, remove spaces)
data.columns = data.columns.str.strip().str.lower()

# Check for missing values
print("Missing values per column:")
print(data.isnull().sum())

# Data cleaning pipeline
data_clean = (
    data
    # Remove rows with critical missing values
    .dropna(subset=["behavior", "performance"])
    # Convert categorical variables
    .assign(
        behavior=lambda df: df["behavior"].astype("category"),
        instructor=lambda df: df["instructor"].astype("category"),
    )
    # Ensure numeric columns are proper type
    .assign(
        performance=lambda df: pd.to_numeric(df["performance"], errors='coerce')
    )
)

print(f"\nOriginal data shape: {data.shape}")
print(f"Cleaned data shape: {data_clean.shape}")
print(f"Rows removed: {data.shape[0] - data_clean.shape[0]}")

# Display cleaned data summary
print("\nCleaned data summary:")
display(data_clean.describe(include='all'))

# Save cleaned data for later use (using project_root path)
processed_path = project_root / "data" / "processed" / "data_processed.csv"
data_clean.to_csv(processed_path, index=False)
```

After cleaning, our dataset contains **`{python} data_clean.shape[0]` observations** across **`{python} data_clean.shape[1]` variables**. We removed **`{python} data.shape[0] - data_clean.shape[0]` rows** due to missing values in critical variables.

::: {.callout-tip}
## Inline Code for Dynamic Results

Notice the inline code in the paragraph above uses Python expressions wrapped in backticks with `{python}` prefix to insert computed values directly into your text. This ensures your narrative automatically updates when data changes.

**Working examples in this document:**

- The mean performance score is `{python} f'{data_clean["performance"].mean():.3f}'`
- We analyzed data from `{python} data_clean['instructor'].nunique()` different instructors

**To use inline code:** Write your Python expression between backticks with the {python} prefix, like this (without the backslashes): `\{python\} your_expression_here`

This is better than hard-coding numbers, which can become outdated if you update your data!
:::

::: {.callout-important}
## Don't Forget to Interpret!

After showing your data cleaning code, **explain your decisions**:

- **Why** did you remove certain rows or handle missing values this way?
- **What** impact do these choices have on your analysis?
- **How** do your cleaning decisions align with your research questions?

Example: "We removed rows with missing performance scores (N=15, 7.5% of data) because these are our primary outcome variable and cannot be reliably imputed. This minimal data loss is acceptable and preserves the integrity of our performance analysis."
:::

::: {.callout-note}